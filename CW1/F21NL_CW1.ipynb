{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQaGMPUu5vQW"
      },
      "outputs": [],
      "source": [
        "!wget -O wikitext-filtered-full.zip \"https://www.dropbox.com/scl/fi/ibd4cmixckghx6hhb361c/wikitext-filtered-full.zip?rlkey=q71cebf0k5fvvwhmcntoswzhq&dl=1\"\n",
        "!wget -O wikitext-filtered-10k.zip \"https://www.dropbox.com/scl/fi/ek174r3sg7qjx0aa9atop/wikitext-filtered-10k.zip?rlkey=zy6jqxv6qsc16lr9qm3ki9uhf&dl=1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0GgyhgwO59KU"
      },
      "outputs": [],
      "source": [
        "!unzip wikitext-filtered-full.zip\n",
        "!unzip wikitext-filtered-10k.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbUoBOOH5_Zy"
      },
      "outputs": [],
      "source": [
        "# !pip install datasets\n",
        "import datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYe6Rq5k6Bgu"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "def load_dataset():\n",
        "  wikitext_small = \"wikitext-filtered-10k\"\n",
        "  wikitext_large = \"wikitext-filtered-full\"\n",
        "\n",
        "  dataset_small = Dataset.load_from_disk(wikitext_small)\n",
        "  dataset_large = Dataset.load_from_disk(wikitext_large)\n",
        "  print(\"wikitext_small: {} docs, wikitext_large: {} docs\".format(len(dataset_small), len(dataset_large)))\n",
        "  return dataset_small, dataset_large\n",
        "\n",
        "wikitext_small, wikitext_large = load_dataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Response Section\n",
        "a.  What are the cosine similarity scores for the following pairs: \n",
        "-  plane / car \n",
        "-  planet / sun \n",
        "-  cup / article \n",
        "-  sugar / approach\n",
        "\n",
        "b.  What is the value of the Spearman correlation coefficients computed in Step 4?\n",
        "\n",
        "c.  How do you interpret each coefficient value with respect to the word similarity task? \n",
        "Are the coefficient values for the two vector space models you created different, and \n",
        "if so, why?\n",
        "\n",
        "d.  What is the value of the Spearman correlation coefficient and how do you interpret it?\n",
        "\n",
        "e.  Create a table of results that summarises your experiments. Tips: In case you have \n",
        "run several experiments with different hyperparameters choose to present the most \n",
        "significant. It might be worth presenting more than one experiment with only a single \n",
        "hyperparameter change if you want to emphasise a striking difference worth \n",
        "discussing. Finally, apart from the Spearman correlation coefficients, make sure you \n",
        "also include the most significant hyperparameters as separate columns (for example, \n",
        "see Table 2 from Merity et al., 2016).\n",
        "\n",
        "f.  Using the table of results from your answer to question g., write a short discussion \n",
        "section that answers the following questions:  \n",
        "i.  Does a bigger corpus yield better representations?   \n",
        "ii.  Does a bigger vocabulary yield better representations?  \n",
        "iii.  Do bigger word vectors yield better representations?  \n",
        "iv.  Does a bigger context window yield better representations? \n",
        "v.  Step 6 requires you to look for the best combination of hyperparameters using \n",
        "the same two datasets for evaluation. Is this a good practice?\n",
        "\n",
        "g.  What are the top-5 analogies for the following configurations: \n",
        "-  man is to woman as king is to ___? \n",
        "-  Athens is to Greece as Rome is to ___? \n",
        "-  reading is to read as playing is to ___? \n",
        "-  Greece is to souvlaki as Italy is to ___? \n",
        "-  airplane is to propeller as car is to ___? \n",
        "Is the top-1 answer always the “correct”? What about the rest of the results?\n",
        "\n",
        "h.  What are the top-5 analogies for the following configurations? Can you identify any \n",
        "gender-based stereotypes? Briefly discuss your findings: \n",
        "-  man is to woman as computer programmer is to ___? \n",
        "-  man is to woman as superstar is to ___? \n",
        "-  man is to woman as guitarist is to ___? \n",
        "-  man is to woman as boss is to ___?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "NLP",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
