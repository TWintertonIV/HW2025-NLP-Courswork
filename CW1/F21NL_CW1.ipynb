{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQaGMPUu5vQW"
      },
      "outputs": [],
      "source": [
        "!wget -O wikitext-filtered-full.zip \"https://www.dropbox.com/scl/fi/ibd4cmixckghx6hhb361c/wikitext-filtered-full.zip?rlkey=q71cebf0k5fvvwhmcntoswzhq&dl=1\"\n",
        "!wget -O wikitext-filtered-10k.zip \"https://www.dropbox.com/scl/fi/ek174r3sg7qjx0aa9atop/wikitext-filtered-10k.zip?rlkey=zy6jqxv6qsc16lr9qm3ki9uhf&dl=1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0GgyhgwO59KU"
      },
      "outputs": [],
      "source": [
        "!unzip wikitext-filtered-full.zip\n",
        "!unzip wikitext-filtered-10k.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbUoBOOH5_Zy"
      },
      "outputs": [],
      "source": [
        "# !pip install datasets\n",
        "import datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYe6Rq5k6Bgu"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "def load_dataset():\n",
        "  wikitext_small = \"wikitext-filtered-10k\"\n",
        "  wikitext_large = \"wikitext-filtered-full\"\n",
        "\n",
        "  dataset_small = Dataset.load_from_disk(wikitext_small)\n",
        "  dataset_large = Dataset.load_from_disk(wikitext_large)\n",
        "  print(\"wikitext_small: {} docs, wikitext_large: {} docs\".format(len(dataset_small), len(dataset_large)))\n",
        "  return dataset_small, dataset_large\n",
        "\n",
        "wikitext_small, wikitext_large = load_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalize_text(text: str) -> str:\n",
        "    \"\"\"Normalize text by removing special characters, extra spaces, and converting to lowercase. \"\"\"\n",
        "    # Remove special characters manually\n",
        "    cleaned_text = \"\".join(char for char in text if char.isalnum() or char.isspace())\n",
        "    # Remove extra spaces\n",
        "    cleaned_text = \" \".join(cleaned_text.split())\n",
        "    # Convert to lowercase\n",
        "    cleaned_text = cleaned_text.lower()\n",
        "    return cleaned_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "START_TOKEN = \"<s>\"\n",
        "END_TOKEN = \"</s>\"\n",
        "\n",
        "def read_corpus(files) -> list[list[str]]:\n",
        "    Return a list of tokenised reviews, each review is a list of words.\n",
        "\n",
        "    # Return a list of lists, where each sub-list is a tokenized review with start and end tokens\n",
        "    # Access the 'text' column of the dataset\n",
        "    return [f\"{START_TOKEN} {normalize_text(line['text'])} {END_TOKEN}\".split(\" \") for line in files]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "wikitext_smallNormalToken = read_corpus(wikitext_small)\n",
        "print(wikitext_smallNormalToken[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "model1 = Word2Vec(sentences=wikitext_smallNormalToken, vector_size=50, window=5, min_count=5, workers=4)\n",
        "model1.save(\"word2vec.model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Importing WordSim-353"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!wget -O wordsim353.zip \"https://gabrilovich.com/resources/data/wordsim353/wordsim353.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!unzip wordsim353.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "wordsim = pd.read_csv(\"combined.csv\")\n",
        "wordsim.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Step 3 Cosine Similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cosine_similarity(model):\n",
        "  word2vecSimNum = []\n",
        "  for i in range(len(wordsim)):\n",
        "    w1 = wordsim.loc[i].get(\"Word 1\")\n",
        "    w2 = wordsim.loc[i].get(\"Word 2\")\n",
        "    if w1 in model.wv and w2 in model.wv:\n",
        "      word2VecMean = model.wv.similarity(w1, w2)\n",
        "      word2vecSimNum.append(word2VecMean)\n",
        "    else:\n",
        "      word2vecSimNum.append(np.nan)\n",
        "  return word2vecSimNum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model1Cosine = cosine_similarity(model1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Question A\n",
        "planeCar = model1.wv.similarity(\"plane\", \"car\")\n",
        "print(planeCar)\n",
        "\n",
        "planetSun = model1.wv.similarity(\"planet\", \"sun\")\n",
        "print(planetSun)\n",
        "\n",
        "cupArticle = model1.wv.similarity(\"cup\", \"article\")\n",
        "print(cupArticle)\n",
        "\n",
        "sugarApproach = model1.wv.similarity(\"sugar\", \"approach\")\n",
        "print(sugarApproach)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Step 4 Evaluate Semantic Relatedness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import scipy.stats as scistats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scistats.spearmanr(wordsim[\"Human (mean)\"], model1Cosine, nan_policy = 'omit')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Repeating for wiki_large"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Normalising and Tokenising\n",
        "wikitext_largeNormalToken = read_corpus(wikitext_large)\n",
        "print(wikitext_largeNormalToken[0])\n",
        "\n",
        "#Training new word2vec model with larger dataset\n",
        "model2 = Word2Vec(sentences=wikitext_largeNormalToken, vector_size=50, window=5, min_count=5, workers=4)\n",
        "model2.save(\"word2vecLarge.model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model2Cosine = cosine_similarity(model2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scistats.spearmanr(wordsim[\"Human (mean)\"], model2Cosine, nan_policy = 'omit')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Question A Large\n",
        "planeCar = model2.wv.similarity(\"plane\", \"car\")\n",
        "print(planeCar)\n",
        "\n",
        "planetSun = model2.wv.similarity(\"planet\", \"sun\")\n",
        "print(planetSun)\n",
        "\n",
        "cupArticle = model2.wv.similarity(\"cup\", \"article\")\n",
        "print(cupArticle)\n",
        "\n",
        "sugarApproach = model2.wv.similarity(\"sugar\", \"approach\")\n",
        "print(sugarApproach)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Step 5 Pre Trained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gensim.downloader as downloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "preTrainedModel = downloader.load(\"word2vec-google-news-300\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cosine_similarity_pretrained(model):\n",
        "\n",
        "    word2vecSimNum = []\n",
        "    for i in range(len(wordsim)):\n",
        "        w1 = wordsim.loc[i].get(\"Word 1\")\n",
        "        w2 = wordsim.loc[i].get(\"Word 2\")\n",
        "        if w1 in model and w2 in model:\n",
        "            word2VecMean = model.similarity(w1, w2)\n",
        "            word2vecSimNum.append(word2VecMean)\n",
        "        else:\n",
        "            word2vecSimNum.append(np.nan)\n",
        "    return word2vecSimNum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "preTrainedCosine = cosine_similarity_pretrained(preTrainedModel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scistats.spearmanr(wordsim[\"Human (mean)\"], preTrainedCosine, nan_policy = 'omit')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Response Section\n",
        "a.  What are the cosine similarity scores for the following pairs: \n",
        "-  plane / car \n",
        "-  planet / sun \n",
        "-  cup / article \n",
        "-  sugar / approach\n",
        "\n",
        "b.  What is the value of the Spearman correlation coefficients computed in Step 4?\n",
        "\n",
        "c.  How do you interpret each coefficient value with respect to the word similarity task? \n",
        "Are the coefficient values for the two vector space models you created different, and \n",
        "if so, why?\n",
        "\n",
        "d.  What is the value of the Spearman correlation coefficient and how do you interpret it?\n",
        "\n",
        "e.  Create a table of results that summarises your experiments. Tips: In case you have \n",
        "run several experiments with different hyperparameters choose to present the most \n",
        "significant. It might be worth presenting more than one experiment with only a single \n",
        "hyperparameter change if you want to emphasise a striking difference worth \n",
        "discussing. Finally, apart from the Spearman correlation coefficients, make sure you \n",
        "also include the most significant hyperparameters as separate columns (for example, \n",
        "see Table 2 from Merity et al., 2016).\n",
        "\n",
        "f.  Using the table of results from your answer to question g., write a short discussion \n",
        "section that answers the following questions:  \n",
        "i.  Does a bigger corpus yield better representations?   \n",
        "ii.  Does a bigger vocabulary yield better representations?  \n",
        "iii.  Do bigger word vectors yield better representations?  \n",
        "iv.  Does a bigger context window yield better representations? \n",
        "v.  Step 6 requires you to look for the best combination of hyperparameters using \n",
        "the same two datasets for evaluation. Is this a good practice?\n",
        "\n",
        "g.  What are the top-5 analogies for the following configurations: \n",
        "-  man is to woman as king is to ___? \n",
        "-  Athens is to Greece as Rome is to ___? \n",
        "-  reading is to read as playing is to ___? \n",
        "-  Greece is to souvlaki as Italy is to ___? \n",
        "-  airplane is to propeller as car is to ___? \n",
        "Is the top-1 answer always the “correct”? What about the rest of the results?\n",
        "\n",
        "h.  What are the top-5 analogies for the following configurations? Can you identify any \n",
        "gender-based stereotypes? Briefly discuss your findings: \n",
        "-  man is to woman as computer programmer is to ___? \n",
        "-  man is to woman as superstar is to ___? \n",
        "-  man is to woman as guitarist is to ___? \n",
        "-  man is to woman as boss is to ___?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "NLP",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
