{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sQaGMPUu5vQW"
   },
   "outputs": [],
   "source": [
    "!wget -O wikitext-filtered-full.zip \"https://www.dropbox.com/scl/fi/ibd4cmixckghx6hhb361c/wikitext-filtered-full.zip?rlkey=q71cebf0k5fvvwhmcntoswzhq&dl=1\"\n",
    "!wget -O wikitext-filtered-10k.zip \"https://www.dropbox.com/scl/fi/ek174r3sg7qjx0aa9atop/wikitext-filtered-10k.zip?rlkey=zy6jqxv6qsc16lr9qm3ki9uhf&dl=1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0GgyhgwO59KU"
   },
   "outputs": [],
   "source": [
    "!unzip wikitext-filtered-full.zip\n",
    "!unzip wikitext-filtered-10k.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gbUoBOOH5_Zy"
   },
   "outputs": [],
   "source": [
    "# !pip install datasets\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "aYe6Rq5k6Bgu"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tom/.local/share/mamba/envs/NLP/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wikitext_small: 10000 docs, wikitext_large: 859955 docs\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def load_dataset():\n",
    "  wikitext_small = \"wikitext-filtered-10k\"\n",
    "  wikitext_large = \"wikitext-filtered-full\"\n",
    "\n",
    "  dataset_small = Dataset.load_from_disk(wikitext_small)\n",
    "  dataset_large = Dataset.load_from_disk(wikitext_large)\n",
    "  print(\"wikitext_small: {} docs, wikitext_large: {} docs\".format(len(dataset_small), len(dataset_large)))\n",
    "  return dataset_small, dataset_large\n",
    "\n",
    "wikitext_small, wikitext_large = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"Normalize text by removing special characters, extra spaces, and converting to lowercase. \"\"\"\n",
    "    # Remove special characters manually\n",
    "    cleaned_text = \"\".join(char for char in text if char.isalnum() or char.isspace())\n",
    "    # Remove extra spaces\n",
    "    cleaned_text = \" \".join(cleaned_text.split())\n",
    "    # Convert to lowercase\n",
    "    cleaned_text = cleaned_text.lower()\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TOKEN = \"<s>\"\n",
    "END_TOKEN = \"</s>\"\n",
    "\n",
    "def read_corpus(files) -> list[list[str]]:\n",
    "    # Return a list of tokenised reviews, each review is a list of words.\n",
    "\n",
    "    # Return a list of lists, where each sub-list is a tokenized review with start and end tokens\n",
    "    # Access the 'text' column of the dataset\n",
    "    return [f\"{START_TOKEN} {normalize_text(line['text'])} {END_TOKEN}\".split(\" \") for line in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'senjō', 'no', 'valkyria', '3', 'unrecorded', 'chronicles', 'japanese', '戦場のヴァルキュリア3', 'lit', 'valkyria', 'of', 'the', 'battlefield', '3', 'commonly', 'referred', 'to', 'as', 'valkyria', 'chronicles', 'iii', 'outside', 'japan', 'is', 'a', 'tactical', 'role', 'playing', 'video', 'game', 'developed', 'by', 'sega', 'and', 'mediavision', 'for', 'the', 'playstation', 'portable', 'released', 'in', 'january', '2011', 'in', 'japan', 'it', 'is', 'the', 'third', 'game', 'in', 'the', 'valkyria', 'series', 'employing', 'the', 'same', 'fusion', 'of', 'tactical', 'and', 'real', 'time', 'gameplay', 'as', 'its', 'predecessors', 'the', 'story', 'runs', 'parallel', 'to', 'the', 'first', 'game', 'and', 'follows', 'the', 'nameless', 'a', 'penal', 'military', 'unit', 'serving', 'the', 'nation', 'of', 'gallia', 'during', 'the', 'second', 'europan', 'war', 'who', 'perform', 'secret', 'black', 'operations', 'and', 'are', 'pitted', 'against', 'the', 'imperial', 'unit', 'calamaty', 'raven', '</s>']\n"
     ]
    }
   ],
   "source": [
    "wikitext_smallNormalToken = read_corpus(wikitext_small)\n",
    "print(wikitext_smallNormalToken[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model1 = Word2Vec(sentences=wikitext_smallNormalToken, vector_size=50, window=5, min_count=5, workers=4)\n",
    "model1.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing WordSim-353"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-10-19 11:13:05--  https://gabrilovich.com/resources/data/wordsim353/wordsim353.zip\n",
      "Resolving gabrilovich.com (gabrilovich.com)... 173.236.137.139\n",
      "Connecting to gabrilovich.com (gabrilovich.com)|173.236.137.139|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 23257 (23K) [application/zip]\n",
      "Saving to: ‘wordsim353.zip’\n",
      "\n",
      "wordsim353.zip      100%[===================>]  22.71K  --.-KB/s    in 0.09s   \n",
      "\n",
      "2025-10-19 11:13:06 (252 KB/s) - ‘wordsim353.zip’ saved [23257/23257]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -O wordsim353.zip \"https://gabrilovich.com/resources/data/wordsim353/wordsim353.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  wordsim353.zip\n",
      "  inflating: combined.csv            \n",
      "  inflating: set1.csv                \n",
      "  inflating: set2.csv                \n",
      "  inflating: combined.tab            \n",
      "  inflating: set1.tab                \n",
      "  inflating: set2.tab                \n",
      "  inflating: instructions.txt        \n"
     ]
    }
   ],
   "source": [
    "!unzip wordsim353.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word 1</th>\n",
       "      <th>Word 2</th>\n",
       "      <th>Human (mean)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>love</td>\n",
       "      <td>sex</td>\n",
       "      <td>6.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tiger</td>\n",
       "      <td>cat</td>\n",
       "      <td>7.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tiger</td>\n",
       "      <td>tiger</td>\n",
       "      <td>10.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>book</td>\n",
       "      <td>paper</td>\n",
       "      <td>7.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>computer</td>\n",
       "      <td>keyboard</td>\n",
       "      <td>7.62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Word 1    Word 2  Human (mean)\n",
       "0      love       sex          6.77\n",
       "1     tiger       cat          7.35\n",
       "2     tiger     tiger         10.00\n",
       "3      book     paper          7.46\n",
       "4  computer  keyboard          7.62"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsim = pd.read_csv(\"combined.csv\")\n",
    "wordsim.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3 Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(model):\n",
    "  word2vecSimNum = []\n",
    "  for i in range(len(wordsim)):\n",
    "    w1 = wordsim.loc[i].get(\"Word 1\")\n",
    "    w2 = wordsim.loc[i].get(\"Word 2\")\n",
    "    if w1 in model.wv and w2 in model.wv:\n",
    "      word2VecMean = model.wv.similarity(w1, w2)\n",
    "      word2vecSimNum.append(word2VecMean)\n",
    "    else:\n",
    "      word2vecSimNum.append(np.nan)\n",
    "  return word2vecSimNum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1Cosine = cosine_similarity(model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8853891\n",
      "0.8932499\n",
      "0.54733366\n",
      "0.7980922\n"
     ]
    }
   ],
   "source": [
    "# Question A\n",
    "planeCar = model1.wv.similarity(\"plane\", \"car\")\n",
    "print(planeCar)\n",
    "\n",
    "planetSun = model1.wv.similarity(\"planet\", \"sun\")\n",
    "print(planetSun)\n",
    "\n",
    "cupArticle = model1.wv.similarity(\"cup\", \"article\")\n",
    "print(cupArticle)\n",
    "\n",
    "sugarApproach = model1.wv.similarity(\"sugar\", \"approach\")\n",
    "print(sugarApproach)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4 Evaluate Semantic Relatedness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as scistats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SignificanceResult(statistic=0.1203052723914138, pvalue=0.05649148344997391)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scistats.spearmanr(wordsim[\"Human (mean)\"], model1Cosine, nan_policy = 'omit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeating for wiki_large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'senjō', 'no', 'valkyria', '3', 'unrecorded', 'chronicles', 'japanese', '戦場のヴァルキュリア3', 'lit', 'valkyria', 'of', 'the', 'battlefield', '3', 'commonly', 'referred', 'to', 'as', 'valkyria', 'chronicles', 'iii', 'outside', 'japan', 'is', 'a', 'tactical', 'role', 'playing', 'video', 'game', 'developed', 'by', 'sega', 'and', 'mediavision', 'for', 'the', 'playstation', 'portable', 'released', 'in', 'january', '2011', 'in', 'japan', 'it', 'is', 'the', 'third', 'game', 'in', 'the', 'valkyria', 'series', 'employing', 'the', 'same', 'fusion', 'of', 'tactical', 'and', 'real', 'time', 'gameplay', 'as', 'its', 'predecessors', 'the', 'story', 'runs', 'parallel', 'to', 'the', 'first', 'game', 'and', 'follows', 'the', 'nameless', 'a', 'penal', 'military', 'unit', 'serving', 'the', 'nation', 'of', 'gallia', 'during', 'the', 'second', 'europan', 'war', 'who', 'perform', 'secret', 'black', 'operations', 'and', 'are', 'pitted', 'against', 'the', 'imperial', 'unit', 'calamaty', 'raven', '</s>']\n"
     ]
    }
   ],
   "source": [
    "#Normalising and Tokenising\n",
    "wikitext_largeNormalToken = read_corpus(wikitext_large)\n",
    "print(wikitext_largeNormalToken[0])\n",
    "\n",
    "#Training new word2vec model with larger dataset\n",
    "model2 = Word2Vec(sentences=wikitext_largeNormalToken, vector_size=50, window=5, min_count=5, workers=4)\n",
    "model2.save(\"word2vecLarge.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2Cosine = cosine_similarity(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SignificanceResult(statistic=0.6074315423385077, pvalue=3.653239708480032e-35)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scistats.spearmanr(wordsim[\"Human (mean)\"], model2Cosine, nan_policy = 'omit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6524783\n",
      "0.63540334\n",
      "0.10199884\n",
      "-0.14861105\n"
     ]
    }
   ],
   "source": [
    "# Question A Large\n",
    "planeCar = model2.wv.similarity(\"plane\", \"car\")\n",
    "print(planeCar)\n",
    "\n",
    "planetSun = model2.wv.similarity(\"planet\", \"sun\")\n",
    "print(planetSun)\n",
    "\n",
    "cupArticle = model2.wv.similarity(\"cup\", \"article\")\n",
    "print(cupArticle)\n",
    "\n",
    "sugarApproach = model2.wv.similarity(\"sugar\", \"approach\")\n",
    "print(sugarApproach)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5 Pre Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "preTrainedModel = downloader.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_pretrained(model):\n",
    "\n",
    "    word2vecSimNum = []\n",
    "    for i in range(len(wordsim)):\n",
    "        w1 = wordsim.loc[i].get(\"Word 1\")\n",
    "        w2 = wordsim.loc[i].get(\"Word 2\")\n",
    "        if w1 in model and w2 in model:\n",
    "            word2VecMean = model.similarity(w1, w2)\n",
    "            word2vecSimNum.append(word2VecMean)\n",
    "        else:\n",
    "            word2vecSimNum.append(np.nan)\n",
    "    return word2vecSimNum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "preTrainedCosine = cosine_similarity_pretrained(preTrainedModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SignificanceResult(statistic=0.7000166486272194, pvalue=2.8686666605142608e-53)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scistats.spearmanr(wordsim[\"Human (mean)\"], preTrainedCosine, nan_policy = 'omit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large Model Hyper-parameter Test\n",
      "Window=5, Vector=50: SignificanceResult(statistic=0.6118080510433198, pvalue=8.821653866113175e-36)\n",
      "Window=5, Vector=50: SignificanceResult(statistic=0.6118080510433198, pvalue=8.821653866113175e-36)\n",
      "Window=10, Vector=50: SignificanceResult(statistic=0.6338436494812426, pvalue=4.8588282405768905e-39)\n",
      "Window=10, Vector=50: SignificanceResult(statistic=0.6338436494812426, pvalue=4.8588282405768905e-39)\n",
      "Window=15, Vector=50: SignificanceResult(statistic=0.6489743805643045, pvalue=1.9562697793877926e-41)\n",
      "Window=15, Vector=50: SignificanceResult(statistic=0.6489743805643045, pvalue=1.9562697793877926e-41)\n",
      "Window=5, Vector=100: SignificanceResult(statistic=0.6279410603944227, pvalue=3.8476799879517744e-38)\n",
      "Window=5, Vector=100: SignificanceResult(statistic=0.6279410603944227, pvalue=3.8476799879517744e-38)\n",
      "Window=10, Vector=100: SignificanceResult(statistic=0.6687969868900207, pvalue=8.697439763044538e-45)\n",
      "Window=10, Vector=100: SignificanceResult(statistic=0.6687969868900207, pvalue=8.697439763044538e-45)\n",
      "Window=15, Vector=100: SignificanceResult(statistic=0.6741698687202118, pvalue=9.677040413801231e-46)\n",
      "Window=15, Vector=100: SignificanceResult(statistic=0.6741698687202118, pvalue=9.677040413801231e-46)\n",
      "Window=5, Vector=200: SignificanceResult(statistic=0.6273690595484579, pvalue=4.6909498557395864e-38)\n",
      "Window=5, Vector=200: SignificanceResult(statistic=0.6273690595484579, pvalue=4.6909498557395864e-38)\n",
      "Window=10, Vector=200: SignificanceResult(statistic=0.6679032355682006, pvalue=1.2476512157501704e-44)\n",
      "Window=10, Vector=200: SignificanceResult(statistic=0.6679032355682006, pvalue=1.2476512157501704e-44)\n",
      "Window=15, Vector=200: SignificanceResult(statistic=0.6801536934182472, pvalue=7.93661054144863e-47)\n",
      "Window=15, Vector=200: SignificanceResult(statistic=0.6801536934182472, pvalue=7.93661054144863e-47)\n"
     ]
    }
   ],
   "source": [
    "#Differing Hyper Paramters test\n",
    "#Large Models\n",
    "# Differing Hyperparameters Test\n",
    "params = [\n",
    "    (5, 50), (10, 50), (15, 50),\n",
    "    (5, 100), (10, 100), (15, 100),\n",
    "    (5, 200), (10, 200), (15, 200)\n",
    "]\n",
    "print(\"Large Model Hyper-parameter Test\")\n",
    "for window, vector_size in params:\n",
    "    model = Word2Vec(sentences=wikitext_largeNormalToken, vector_size=vector_size, window=window, min_count=5, workers=4)\n",
    "    score = scistats.spearmanr(wordsim[\"Human (mean)\"], cosine_similarity(model), nan_policy='omit')\n",
    "    print(f'Window={window}, Vector={vector_size}: {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Small Model Hyper-parameter Test\n",
      "Window=5, Vector=50: SignificanceResult(statistic=0.0993008784103129, pvalue=0.11585684219911782)\n",
      "Window=5, Vector=50: SignificanceResult(statistic=0.0993008784103129, pvalue=0.11585684219911782)\n",
      "Window=10, Vector=50: SignificanceResult(statistic=0.15408177770011589, pvalue=0.014347198561918623)\n",
      "Window=10, Vector=50: SignificanceResult(statistic=0.15408177770011589, pvalue=0.014347198561918623)\n",
      "Window=15, Vector=50: SignificanceResult(statistic=0.20274245701573984, pvalue=0.001211709851426948)\n",
      "Window=15, Vector=50: SignificanceResult(statistic=0.20274245701573984, pvalue=0.001211709851426948)\n",
      "Window=5, Vector=100: SignificanceResult(statistic=0.05373011717391517, pvalue=0.3957075259348354)\n",
      "Window=5, Vector=100: SignificanceResult(statistic=0.05373011717391517, pvalue=0.3957075259348354)\n",
      "Window=10, Vector=100: SignificanceResult(statistic=0.15254300524566322, pvalue=0.015362988294003371)\n",
      "Window=10, Vector=100: SignificanceResult(statistic=0.15254300524566322, pvalue=0.015362988294003371)\n",
      "Window=15, Vector=100: SignificanceResult(statistic=0.19068424012119853, pvalue=0.0023664849070187143)\n",
      "Window=15, Vector=100: SignificanceResult(statistic=0.19068424012119853, pvalue=0.0023664849070187143)\n",
      "Window=5, Vector=200: SignificanceResult(statistic=0.09405802869525161, pvalue=0.13648618699768508)\n",
      "Window=5, Vector=200: SignificanceResult(statistic=0.09405802869525161, pvalue=0.13648618699768508)\n",
      "Window=10, Vector=200: SignificanceResult(statistic=0.13080821927107458, pvalue=0.03797571679422249)\n",
      "Window=10, Vector=200: SignificanceResult(statistic=0.13080821927107458, pvalue=0.03797571679422249)\n",
      "Window=15, Vector=200: SignificanceResult(statistic=0.1415571297789325, pvalue=0.024620473519626044)\n",
      "Window=15, Vector=200: SignificanceResult(statistic=0.1415571297789325, pvalue=0.024620473519626044)\n"
     ]
    }
   ],
   "source": [
    "#Differing Hyper Paramters test\n",
    "#Small Models\n",
    "# Differing Hyperparameters Test\n",
    "params = [\n",
    "    (5, 50), (10, 50), (15, 50),\n",
    "    (5, 100), (10, 100), (15, 100),\n",
    "    (5, 200), (10, 200), (15, 200)\n",
    "]\n",
    "print(\"Small Model Hyper-parameter Test\")\n",
    "for (window, vector_size) in params:\n",
    "    model = Word2Vec(sentences=wikitext_smallNormalToken, vector_size=vector_size, window=window, min_count=5, workers=4)\n",
    "    score = scistats.spearmanr(wordsim[\"Human (mean)\"], cosine_similarity(model), nan_policy='omit')\n",
    "    print(f'Window={window}, Vector={vector_size}: {score}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTopKAnalogyResult(base1, analogy1, base2, modelVW, k=5):\n",
    "    \"\"\"\n",
    "    Find top-k words for analogy: base1 is to analogy1 as base2 is to ____\n",
    "    Returns a list of top-k words (excluding input words).\n",
    "    \"\"\"\n",
    "    \n",
    "    if not all(w in modelVW for w in [base1, analogy1, base2]):\n",
    "        raise ValueError(\"One or more words not in vocabulary.\")\n",
    "    analogy_vector = modelVW[analogy1] - modelVW[base1] + modelVW[base2]\n",
    "    # Find top k most similar words to the analogy vector\n",
    "    results = modelVW.similar_by_vector(analogy_vector, topn=k+3)\n",
    "    # Exclude the input words from the results\n",
    "    filtered = [word for word, score in results if word not in {base1, analogy1, base2}]\n",
    "    return filtered[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['queen', 'monarch', 'princess', 'crown_prince', 'prince']\n",
      "['Italy', 'Sicily', 'Portugal', 'Italian', 'ANSA']\n",
      "['played', 'play', 'Playing', 'playin', 'toplay']\n",
      "['quiche_Lorraine', 'banh_mi_sandwiches', 'porchetta', 'veal_parmigiana', 'panino']\n",
      "['steering_wheel', 'front_fender', 'fender', 'brake_rotor', 'headlight']\n"
     ]
    }
   ],
   "source": [
    "# man is to woman as king is to ___?\n",
    "print(getTopKAnalogyResult(\"man\", \"woman\", \"king\", preTrainedModel))\n",
    "# Athens is to Greece as Rome is to ___?\n",
    "print(getTopKAnalogyResult(\"Athens\", \"Greece\", \"Rome\", preTrainedModel))\n",
    "# reading is to read as playing is to ___? \n",
    "print(getTopKAnalogyResult(\"reading\", \"read\", \"playing\", preTrainedModel))\n",
    "# Greece is to souvlaki as Italy is to ___? \n",
    "print(getTopKAnalogyResult(\"Greece\", \"souvlaki\", \"Italy\", preTrainedModel))\n",
    "# airplane is to propeller as car is to ___? \n",
    "print(getTopKAnalogyResult(\"airplane\", \"propeller\", \"car\", preTrainedModel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['programmers', 'computer_programmer', 'coder', 'Programmer', 'programer']\n",
      "['megastar', 'diva', 'star', 'pop_diva', 'songstress']\n",
      "['vocalist', 'drummer', 'bassist', 'singer_guitarist', 'guitarist_vocalist']\n",
      "['bosses', 'manageress', 'coworker', 'receptionist', 'exec']\n"
     ]
    }
   ],
   "source": [
    "# man is to woman as computer programmer is to ___?\n",
    "print(getTopKAnalogyResult(\"man\", \"woman\", \"programmer\", preTrainedModel))\n",
    "# man is to woman as superstar is to ___?\n",
    "print(getTopKAnalogyResult(\"man\", \"woman\", \"superstar\", preTrainedModel))\n",
    "# man is to woman as guitarist is to ___? \n",
    "print(getTopKAnalogyResult(\"man\", \"woman\", \"guitarist\", preTrainedModel))\n",
    "# man is to woman as boss is to ___?  \n",
    "print(getTopKAnalogyResult(\"man\", \"woman\", \"boss\", preTrainedModel))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Response Section\n",
    "a.  What are the cosine similarity scores for the following pairs: \n",
    "\n",
    "-  plane / car \n",
    "-  planet / sun \n",
    "-  cup / article \n",
    "-  sugar / approach\n",
    "\n",
    "---\n",
    "\n",
    "wikitext-small\n",
    "\n",
    "plane / car\n",
    "0.8853891\n",
    "\n",
    "planet / sun\n",
    "0.8932499\n",
    "\n",
    "cup/article\n",
    "0.54733366\n",
    "\n",
    "sugar / approach\n",
    "0.7980922\n",
    "\n",
    "\n",
    "wikitext-large\n",
    "\n",
    "plane / car\n",
    "0.6524783\n",
    "\n",
    "planet / sun\n",
    "0.63540334\n",
    "\n",
    "cup/article\n",
    "0.10199884\n",
    "\n",
    "sugar / approach\n",
    "-0.14861105\n",
    "\n",
    "---\n",
    "\n",
    "b.  What is the value of the Spearman correlation coefficients computed in Step 4?\n",
    "\n",
    "---\n",
    "\n",
    "wikitext-small\n",
    "Value = .1203\n",
    "\n",
    "wikitext-large\n",
    "Value = .60743\n",
    "\n",
    "---\n",
    "\n",
    "c.  How do you interpret each coefficient value with respect to the word similarity task? \n",
    "Are the coefficient values for the two vector space models you created different, and \n",
    "if so, why?\n",
    "\n",
    "---\n",
    "\n",
    "You can interpret the coefficient value in terms of the word similarity task as a measure for how similarly the model performs in regards of ranking the similarity of word pairs compared to humans ranking the same pairs of words. If the coefficient scores highly, then that means that the model is computing similarities in a manner that is similar to how our group of humans percieve the similarity of the given words. Thus, we can say that the value of .1203 for the small wikitext vector space model and .60743 for the large wikitext vector space model shows that the ranking coorelation between the models' performance and the human control group is .1203 and.60743 respectively.\n",
    "\n",
    "These values are different, due to the reduction in bias that occurs as you gain more data in a larger corpus. When less data is available word correlations that are not representative of a language as a whole may occur and skew data, whereas when a larger corpus is present individual abnormal word correlations impact the overall learning of the similarity less.\n",
    "\n",
    "---\n",
    "\n",
    "d.  What is the value of the Spearman correlation coefficient and how do you interpret it?\n",
    "\n",
    "---\n",
    "\n",
    "pretrained\n",
    "Value = .7000\n",
    "\n",
    "You can intepret this correlation coefficient to mean that there is a moderately strong positive correlation between the rankings of similarities of words between the model and human control group, coming in at .70 alignment.\n",
    "\n",
    "---\n",
    "\n",
    "e.  Create a table of results that summarises your experiments. \n",
    "Tips: In case you have \n",
    "run several experiments with different hyperparameters choose to present the most \n",
    "significant. It might be worth presenting more than one experiment with only a single \n",
    "hyperparameter change if you want to emphasise a striking difference worth \n",
    "discussing. Finally, apart from the Spearman correlation coefficients, make sure you \n",
    "also include the most significant hyperparameters as separate columns (for example, \n",
    "see Table 2 from Merity et al., 2016).\n",
    "\n",
    "---\n",
    "\n",
    "_Table formatting generated with help from ChatGPT_\n",
    "\n",
    "*All Experiments done with a standard 5 word minimum count in corpus for vector generator*\n",
    "\n",
    "### Large Corpus\n",
    "\n",
    "| Vector Size | Window Size = 5 | Window Size = 10 | Window Size = 15 |\n",
    "|--------------|-----------------|------------------|------------------|\n",
    "| 50           | 0.6118          | 0.6338           | 0.6490           |\n",
    "| 100          | 0.6279          | 0.6688           | 0.6742           |\n",
    "| 200          | 0.6274          | 0.6679           | 0.6802           |\n",
    "\n",
    "\n",
    "\n",
    "### Small Corpus\n",
    "\n",
    "| Vector Size | Window Size = 5 | Window Size = 10 | Window Size = 15 |\n",
    "|--------------|-----------------|------------------|------------------|\n",
    "| 50           | 0.1213          | 0.1523           | 0.1939           |\n",
    "| 100          | 0.0625          | 0.1443           | 0.1792           |\n",
    "| 200          | 0.1056          | 0.1580           | 0.1714           |\n",
    "\n",
    "\n",
    "### Combined View\n",
    "\n",
    "| Dataset | Vector Size | Window Size | Coefficient Score |\n",
    "|----------|--------------|--------------|--------------------|\n",
    "| Large    | 50           | 5            | 0.6118             |\n",
    "| Large    | 50           | 10           | 0.6338             |\n",
    "| Large    | 50           | 15           | 0.6490             |\n",
    "| Large    | 100          | 5            | 0.6279             |\n",
    "| Large    | 100          | 10           | 0.6688             |\n",
    "| Large    | 100          | 15           | 0.6742             |\n",
    "| Large    | 200          | 5            | 0.6274             |\n",
    "| Large    | 200          | 10           | 0.6679             |\n",
    "| Large    | 200          | 15           | 0.6802             |\n",
    "| Small    | 50           | 5            | 0.1213             |\n",
    "| Small    | 50           | 10           | 0.1523             |\n",
    "| Small    | 50           | 15           | 0.1939             |\n",
    "| Small    | 100          | 5            | 0.0625             |\n",
    "| Small    | 100          | 10           | 0.1443             |\n",
    "| Small    | 100          | 15           | 0.1792             |\n",
    "| Small    | 200          | 5            | 0.1056             |\n",
    "| Small    | 200          | 10           | 0.1580             |\n",
    "| Small    | 200          | 15           | 0.1714             |\n",
    "\n",
    "---\n",
    "\n",
    "f.  Using the table of results from your answer to question g., write a short discussion section that answers the following questions:  \n",
    "i.  Does a bigger corpus yield better representations?   \n",
    "ii.  Does a bigger vocabulary yield better representations?  \n",
    "iii.  Do bigger word vectors yield better representations?  \n",
    "iv.  Does a bigger context window yield better representations? \n",
    "v.  Step 6 requires you to look for the best combination of hyperparameters using \n",
    "the same two datasets for evaluation. Is this a good practice?\n",
    "\n",
    "---\n",
    "\n",
    "There are several trends and conclusions that we can draw using the data collected through the experiments done here. Firstly, training a Word2Vec model over a larger corpus generally leads to much more accurate representations, as seen through the massively higher coefficient scores for all of our large model experiments compared to our smaller dataset. That being said, a larger vocabularly doesn't necessarily lead to better representations, as for the most part extremely rare words don't tend to influence our understanding of most words outside of very specific contexts. \n",
    "\n",
    "Moving onto the hyperparameter analysis, we see from our results several factors in play. Firstly, it would seem that having a larger vector for words does seem to give a marginal improvement for these datasets, although it seems to have diminishing returns as we increase to 200 compnents per vector. A notable exception is in our small dataset, which seems to provide the best results with a mere 50 components. This would make sense, as with less training data we are less likely to capture specific nuances of words. Thus, the extra detail the larger vectors gives us is unneeded. Moving onto our window hyperparameter, we see a similar correlation where larger windows seem to give us better results. As we increase the number of words in our context window, we do see a marginal improvement for the correlation scores for this dataset. It should be noted however, that as window size increases we lose semantic meaning of individual words, and learn more about the context of the document. \n",
    "\n",
    "Lastly, it should be understood that when searching for general purpose representations of words, you should avoid looking for best combinations of hyperparameters based on two datasets. While we see positive results from certain combinations of hyperparameters for these two datasets, it cannot be concluded that these hyperparameters are the best ones for any arbitrary dataset.\n",
    "\n",
    "---\n",
    "\n",
    "g.  What are the top-5 analogies for the following configurations: \n",
    "-  man is to woman as king is to ___? \n",
    "-  Athens is to Greece as Rome is to ___? \n",
    "-  reading is to read as playing is to ___? \n",
    "-  Greece is to souvlaki as Italy is to ___? \n",
    "-  airplane is to propeller as car is to ___? \n",
    "Is the top-1 answer always the “correct”? What about the rest of the results?\n",
    "\n",
    "---\n",
    "\n",
    "man is to woman as king is to ___?\n",
    "['queen', 'monarch', 'princess', 'crown_prince', 'prince']\n",
    "\n",
    "Athens is to Greece as Rome is to ___?\n",
    "['Italy', 'Sicily', 'Portugal', 'Italian', 'ANSA']\n",
    "\n",
    "reading is to read as playing is to ___?\n",
    "['played', 'play', 'Playing', 'playin', 'toplay']\n",
    "\n",
    "Greece is to souvlaki as Italy is to ___?\n",
    "['quiche_Lorraine', 'banh_mi_sandwiches', 'porchetta', 'veal_parmigiana', 'panino']\n",
    "\n",
    "airplane is to propeller as car is to ___?\n",
    "['steering_wheel', 'front_fender', 'fender', 'brake_rotor', 'headlight']\n",
    "\n",
    "The top-1 answer is not always correct, as sometimes context and differing meanings can make other answers just as valid. Take in consideration 'reading is to read as playing is to _'. For this analogy read can be either present or past tense, thus both played and play are valid answers. The rest of the results are generally somewhat correct or related, but the Top-1 generally does a good job at answering the analogies.\n",
    "\n",
    "---\n",
    "\n",
    "h.  What are the top-5 analogies for the following configurations? Can you identify any \n",
    "gender-based stereotypes? Briefly discuss your findings: \n",
    "-  man is to woman as computer programmer is to ___? \n",
    "-  man is to woman as superstar is to ___? \n",
    "-  man is to woman as guitarist is to ___? \n",
    "-  man is to woman as boss is to ___?\n",
    "\n",
    "---\n",
    "\n",
    "man is to woman as computer programmer is to ___?\n",
    "['programmers', 'computer_programmer', 'coder', 'Programmer', 'programer']\n",
    "\n",
    "man is to woman as superstar is to ___?\n",
    "['megastar', 'diva', 'star', 'pop_diva', 'songstress']\n",
    "\n",
    "man is to woman as guitarist is to ___?\n",
    "['vocalist', 'drummer', 'bassist', 'singer_guitarist', 'guitarist_vocalist']\n",
    "\n",
    "man is to woman as boss is to ___?\n",
    "['bosses', 'manageress', 'coworker', 'receptionist', 'exec']\n",
    "\n",
    "\n",
    "There are a few gender-based stereotypes present in these results, such as 'sonstress', 'vocalist', 'coworker', and 'receptionist'. All of these terms are related to the prompt, but tend to have connotations that are less in regard than the original title. This goes to show how in the era of AI we must be careful to not propogate these stereotypes unconciously in our language, as it can influence and bias important systems in ways that disadvantage certain groups.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
